직접 만든 코드에서 최적의 배치 사이즈를 찾기 위해, 4, 8, 10 순서대로 늘려서 8GB VRAM에서 배치사이즈가 10일때 메모리를 최대로 활용한다는 것을 확인했습니다.

총 14 Epoch를 돌렸고, Validation은 25 -> 117 -> 8 -> 3.5 -> 2.6 -> 5.1 -> 2.9 -> 2.2 -> 1.4 -> 1.3 -> 1.9 -> 1.9 -> 1.9 -> 2.3 으로 나왔습니다.

처음에는 Validation Loss가 점점 줄어들다가 후반에 갈 수록 오히려 올라가는 모습을 보입니다.

이에 다음과 같은 과정을 진행하고자 합니다.

1. ReduceLROnPlateau 스케줄러를 사용하여 검증 손실이 정체될 때 자동으로 학습률을 낮추기
2. 옵티마이저에 weight decay를 추가하여 모델의 가중치가 너무 커지지 않도록 규제하기
3. 데이터 증강하기
4. 검증 데이터셋 비율을 더 늘리기

1번과 2번 방법까지 진행했을 때는 눈에 띄게 Validation Loss가 줄어들지 않았는데, 3번인 훈련 데이터만을 Albumentations로 증강시켰더니 Training Loss와 Validation Loss가 매 Epoch마다 줄어들기 시작했습니다.

최저 loss인 10 epoch부터 다시 훈련을 시작해서, 1.3 -> 0.7 -> 0.58 -> 0.54 -> 0.50 [14 epoch] -> 0.43
