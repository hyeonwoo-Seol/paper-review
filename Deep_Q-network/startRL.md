## 강화 학습이란?
Agent가 환경과 상호작용하면서 행동을 선택하고 보상을 통해 최적의 정책(policy)를 학습하는 기계학습의 한 분야입니다.

Agent란, 환경과 상호작용해서 상태를 관찰하고 행동을 선택 및 실행하는 주체입니다.

보상이란, Agent가 행동을 취한 결과에 따라 환경으로부터 받는 Scalar 값입니다. 이 보상 Scalar 값을 최대화 하는 방향으로 학습이 진행됩니다.

정책이란, 상태 s를 입력으로 받아서 행동 a를 선택하는 함수 또는 확률 분포입니다.

상태란, Agent가 환경과 상호작용 하는 순간순간의 "환경이 가진 정보 전체를 말합니다. 이 논문에서는 게임 화면 프레임 4장을 상태로 정의했습니다.

## 기존의 강화학습 한계
사람이 직접 설계한 특징이나 센서 정보가 저차원 상태공간에서만 성공적으로 학습하고, raw pixel과 같은 시각적 입력 처리에 어려움이 있습니다.

## DQN 등장 배경
Deep Convolutional Network를 사용합니다. 그리고 end-to-end 강화학습을 통해 영상 프레임을 사람이 직접 특징을 설계하지 않고 신경망에 픽셀값을 입력합니다.

## DQN의 3대 핵심 기법
1. Experience Replay는 과거 경험을 메모리에 저장하고 무작위로 샘플링하여 학습하게 합니다. 이를 통해 샘플 간 상관관계를 줄이고 데이터의 효율성을 높이며 데이터 발산을 방지합니다.
2. Target Network는 일정 간격으로만 목표 네트워크 파라미터를 갱신합니다. 이를 통해 학습 안정화 효과를 얻을 수 있습니다.
3. Reward Clipping은 보상을 1, -1, 0으로 변환하는 것입니다. 이를 통해 오차 기울기 스케일의 안정화와 학습률 통일 이라는 장점을 얻을 수 있습니다.

## 주요 성과
Atari 2600 게임 플렛폼에 존재하는 49종의 게임에서 전문 인간 테스터 수준을 달성했습니다. 동일한 구조와 하이퍼파라미터로 다양한 게임에서 강력한 성능을 보였습니다.

## Markov Decision Process, MDP)

## Markov Property

## Bellman Equation

## Exploration and Exploitation

## Model-Free and Model-Based

## Function Approximation


