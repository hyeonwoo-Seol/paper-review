## 강화 학습이란?
Agent가 환경과 상호작용하면서 행동을 선택하고 보상을 통해 최적의 정책(policy)를 학습하는 기계학습의 한 분야입니다.

Agent란, 환경과 상호작용해서 상태를 관찰하고 행동을 선택 및 실행하는 주체입니다.

보상이란, Agent가 행동을 취한 결과에 따라 환경으로부터 받는 Scalar 값입니다. 이 보상 Scalar 값을 최대화 하는 방향으로 학습이 진행됩니다.

정책이란, 상태 s를 입력으로 받아서 행동 a를 선택하는 함수 또는 확률 분포입니다.

상태란, Agent가 환경과 상호작용 하는 순간순간의 "환경이 가진 정보 전체를 말합니다. 이 논문에서는 게임 화면 프레임 4장을 상태로 정의했습니다.

## 기존의 강화학습 한계
사람이 직접 설계한 특징이나 센서 정보가 저차원 상태공간에서만 성공적으로 학습하고, raw pixel과 같은 시각적 입력 처리에 어려움이 있습니다.

## DQN 등장 배경
Deep Convolutional Network를 사용합니다. 그리고 end-to-end 강화학습을 통해 영상 프레임을 사람이 직접 특징을 설계하지 않고 신경망에 픽셀값을 입력합니다.

## DQN의 3대 핵심 기법
1. Experience Replay는 과거 경험을 메모리에 저장하고 무작위로 샘플링하여 학습하게 합니다. 이를 통해 샘플 간 상관관계를 줄이고 데이터의 효율성을 높이며 데이터 발산을 방지합니다.
2. Target Network는 일정 간격으로만 목표 네트워크 파라미터를 갱신합니다. 이를 통해 학습 안정화 효과를 얻을 수 있습니다.
3. Reward Clipping은 보상을 1, -1, 0으로 변환하는 것입니다. 이를 통해 오차 기울기 스케일의 안정화와 학습률 통일 이라는 장점을 얻을 수 있습니다.

## 주요 성과
Atari 2600 게임 플렛폼에 존재하는 49종의 게임에서 전문 인간 테스터 수준을 달성했습니다. 동일한 구조와 하이퍼파라미터로 다양한 게임에서 강력한 성능을 보였습니다.

## Markov Decision Process, MDP)
강화학습이 이론적 기반이 되는 수학 모델입니다.
상태공간 s, 행동공간 a, 전이확률 P, 보상함수 R, 할인율 gamma 를 정의하고 있습니다.

상태공간 s는 Agent가 처할 수 있는 모든 상황들의 집합입니다. 예를 들어 로봇 청소기의 경우, 방의 각 격자 위치와 배터리 잔량 단계등이 있습니다.

행동공간 a는 Agent가 각 상태 s에서 선택할 수 있는 모든 행동들의 집합입니다. 로봇 청소기는 한 칸 앞으로 이동, 좌회전, 충전기 위치로 이동 등이 있습니다.

전이 확률은 특정 상태 s에서 행동 a를 취했을 때 다음 상태 s'로 전이될 확률을 말합니다. Markov 속성에서, 다음 상태는 현재 상태와 현재 행동에만 의존하고 이전의 과거 행동과 상태는 무시합니다.

보상함수는 상태 s에서 행동 a를 취했을 때 즉시 받는 보상을 수치로 나타낸 것입니다. 

할인률은 미래 보상의 현재 가치를 줄이기 위해 사용하는 계수이고, 0과 1 사이입니다. 0에 가까우면 즉시 보상에만 집중하고, 1에 가까우면 먼 미래의 보상도 즉시 보상과 동일하게 고려하겠다는 뜻입니다.

## Markov Property
마르코프 성질이란, 현재 상태만 알면 이전 모든 과거 상태들은 더 이상 미래에 대한 예측에 추가 정보가 되지 않는다는 기억 상실(memoryless) 특성입니다.

예를 들어, 오늘이 맑음이라는 날씨 상태를 알고 있다면, 과거나 이틀 전 날씨가 비가 오거나 흐린 상태가 더 이상 내일 날씨 예측에 필요 없다는 것을 의미합니다.

이 성질은 모델을 단순화시킬 수 있다는 장점이 있습니다. 하지만 실제 환경에서 Agent가 완전한 상태를 관찰하지 못하고 일부 정보만 얻는 경우 이 성질이 성립하지 않을 수 있고, 자연어 처리나 시계열 예측처럼 과거의 맥락이 중요할 때는 Transformer 구조가 필요할 수 있습니다.

## Bellman Equation
어떤 상태에서 value를 그 다음 상태들의 value로 재귀적으로 표현하는 수식입니다. 즉, 지금 상태의 value = 즉시 얻는 보상 + 다음 상태 value의 기대값입니다.

상태 value 함수는 정책 pi를 따를 때, 상태 s에 있을 때 기대되는 누적 할인 보상으로, ![eq1](startRL/eq1.png) 수식입니다.

행동 value 함수는 상태s에서 행동 a를 취한 후 얻게 되는 기대 누적 할인 보상으로, ![eq2](startRL/eq2.png) 수식입니다.

이를 통해 bellman equation을 구하면, ![eq3](startRL/eq3.png) 입니다. 이 수식은 상태 s에서 모든 가능한 행동 a에 대해, 행동 선택 확률 pi(a|s)를 곱하고 해당 행동으로 s'에 도착할 확률 P(s' | s, a )를 곱한 뒤, 즉시 보상 R(s,a)와 다음 상태 value gammaV(s')의 합을 더해 모두 합산합니다.

## Exploration and Exploitation

## Model-Free and Model-Based

## Function Approximation


